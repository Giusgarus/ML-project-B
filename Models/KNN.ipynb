{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import itertools\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hyperparameter_combinations(param_ranges):\n",
    "    '''\n",
    "    Parameters:\n",
    "    param_ranges (dict): Dictionary with hyperparameter names as keys.\n",
    "                         Each value is a tuple (start, stop, step) indicating the range and step size for the hyperparameter.\n",
    "    Returns:\n",
    "    list: List of dictionaries with all possible combinations of hyperparameters.\n",
    "    '''\n",
    "    param_values = {\n",
    "        key: np.arange(start, stop + step, step)\n",
    "        for key, (start, stop, step) in param_ranges.items()\n",
    "    }\n",
    "    \n",
    "    param_combinations = list(itertools.product(*param_values.values()))\n",
    "    return [\n",
    "        dict(zip(param_values.keys(), combination))\n",
    "        for combination in param_combinations\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "encoder = OneHotEncoder(categories='auto', sparse_output=False)\n",
    "\n",
    "# Load training and test files for each dataset from the specified path\n",
    "monk1_train = pd.read_csv('../Datasets/Monks/monks-1.train', sep='\\s+', header=None)\n",
    "monk1_test = pd.read_csv('../Datasets/Monks/monks-1.test', sep='\\s+', header=None)\n",
    "\n",
    "monk2_train = pd.read_csv('../Datasets/Monks/monks-2.train', sep='\\s+', header=None)\n",
    "monk2_test = pd.read_csv('../Datasets/Monks/monks-2.test', sep='\\s+', header=None)\n",
    "\n",
    "monk3_train = pd.read_csv('../Datasets/Monks/monks-3.train', sep='\\s+', header=None)\n",
    "monk3_test = pd.read_csv('../Datasets/Monks/monks-3.test', sep='\\s+', header=None)\n",
    "\n",
    "# List to store the transformed datasets\n",
    "monks_train = []\n",
    "monks_test = []\n",
    "\n",
    "# Dataset monk1\n",
    "X1_train = monk1_train.iloc[:, 1:7].values  # Features\n",
    "y1_train = monk1_train.iloc[:, 0].values    # Labels\n",
    "\n",
    "X1_test = monk1_test.iloc[:, 1:7].values\n",
    "y1_test = monk1_test.iloc[:, 0].values\n",
    "\n",
    "# Apply encoder to monk1\n",
    "X1_train_encoded = encoder.fit_transform(X1_train)  # Fit and transform on training data\n",
    "X1_test_encoded = encoder.transform(X1_test)        # Only transform on test data\n",
    "\n",
    "monks_train.append((X1_train_encoded, y1_train))\n",
    "monks_test.append((X1_test_encoded, y1_test))\n",
    "\n",
    "# Dataset monk2\n",
    "X2_train = monk2_train.iloc[:, 1:7].values\n",
    "y2_train = monk2_train.iloc[:, 0].values\n",
    "\n",
    "X2_test = monk2_test.iloc[:, 1:7].values\n",
    "y2_test = monk2_test.iloc[:, 0].values\n",
    "\n",
    "# Apply encoder to monk2\n",
    "X2_train_encoded = encoder.fit_transform(X2_train)\n",
    "X2_test_encoded = encoder.transform(X2_test)\n",
    "\n",
    "monks_train.append((X2_train_encoded, y2_train))\n",
    "monks_test.append((X2_test_encoded, y2_test))\n",
    "\n",
    "# Dataset monk3\n",
    "X3_train = monk3_train.iloc[:, 1:7].values\n",
    "y3_train = monk3_train.iloc[:, 0].values\n",
    "\n",
    "X3_test = monk3_test.iloc[:, 1:7].values\n",
    "y3_test = monk3_test.iloc[:, 0].values\n",
    "\n",
    "# Apply encoder to monk3\n",
    "X3_train_encoded = encoder.fit_transform(X3_train)\n",
    "X3_test_encoded = encoder.transform(X3_test)\n",
    "\n",
    "monks_train.append((X3_train_encoded, y3_train))\n",
    "monks_test.append((X3_test_encoded, y3_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_KNN(K=100, metric='euclidean', p = 3, weights='distance'):\n",
    "    '''\n",
    "    Create an K-NN model with the parameter K.\n",
    "    param K: Regularization parameter.\n",
    "    return: K-NN model.\n",
    "    '''\n",
    "    if metric == 'minkowski':\n",
    "        return KNeighborsClassifier(n_neighbors=K, weights=weights, metric=metric, p=p)\n",
    "    else:\n",
    "        return KNeighborsClassifier(n_neighbors=K, weights=weights, metric=metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_k_fold_cross_validation(data, labels, type='euclidean', outer_k=5, inner_k=5, param_grid=None):\n",
    "    \"\"\"\n",
    "    Implements Double K-Fold Cross-Validation\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "     -   data (np.ndarray): Features of the dataset.\n",
    "     -   labels (np.ndarray): Labels of the dataset.\n",
    "     -   type (str): Distance metric type for KNN.\n",
    "     -   outer_k (int): Number of folds for outer cross-validation.\n",
    "     -   inner_k (int): Number of folds for inner cross-validation.\n",
    "     -   param_grid (list): List of dictionaries with hyperparameters to try.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "     -   list: List of scores obtained for each outer fold.\n",
    "     -   list: List of best parameters for each outer fold.\n",
    "    \"\"\"\n",
    "    outer_scores = []\n",
    "    outer_params = []\n",
    "    \n",
    "    # Configuration of the outer k-fold cross-validation\n",
    "    out_kfold = StratifiedKFold(n_splits=outer_k, shuffle=True, random_state=42)\n",
    "\n",
    "    # Outer cross-validation loop\n",
    "    out_fold_no = 1\n",
    "    for train_index, val_index in out_kfold.split(data, labels):\n",
    "        \n",
    "        # Split the dataset into training and validation sets for the outer fold\n",
    "        out_X_train, out_X_val = data[train_index], data[val_index]\n",
    "        out_y_train, out_y_val = labels[train_index], labels[val_index]\n",
    "        \n",
    "        best_params = {}\n",
    "        best_score = -np.inf\n",
    "\n",
    "        # Iterate over each set of hyperparameters in the parameter grid\n",
    "        for params in param_grid:\n",
    "\n",
    "            inner_scores = []\n",
    "\n",
    "            # Inner cross-validation loop\n",
    "            inner_fold_no = 1\n",
    "            inner_kfold = StratifiedKFold(n_splits=inner_k, shuffle=True, random_state=42)\n",
    "\n",
    "            for train_index, val_index in inner_kfold.split(out_X_train, out_y_train):\n",
    "                \n",
    "                # Split the dataset into training and validation sets for the inner fold\n",
    "                inner_X_train, inner_X_val = out_X_train[train_index], out_X_train[val_index]\n",
    "                inner_y_train, inner_y_val = out_y_train[train_index], out_y_train[val_index]\n",
    "\n",
    "                # Create the KNN model with the current set of hyperparameters\n",
    "                model = create_KNN(K=params['K'], metric=type, p=params['P'])\n",
    "                \n",
    "                # Train the model on the training set\n",
    "                model.fit(inner_X_train, inner_y_train)\n",
    "                \n",
    "                # Predict on the validation set\n",
    "                pred = model.predict(inner_X_val)\n",
    "                \n",
    "                # Get the accuracy score for the current fold\n",
    "                score = accuracy_score(pred, inner_y_val)\n",
    "                \n",
    "                # Append the validation accuracy to inner scores\n",
    "                inner_scores.append(score)\n",
    "                inner_fold_no += 1\n",
    "            \n",
    "            # Calculate the average score for the current set of hyperparameters\n",
    "            avg_score = np.mean(inner_scores)\n",
    "\n",
    "            # Update the best score and parameters if the current average score is better\n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                best_params = params\n",
    "\n",
    "        # Create the KNN model with the best hyperparameters found in the inner loop\n",
    "        model = create_KNN(K=best_params['K'], metric=type, p=best_params['P'])\n",
    "                \n",
    "        # Train the model on the training set\n",
    "        model.fit(out_X_train, out_y_train)\n",
    "                \n",
    "        # Predict on the validation set\n",
    "        pred = model.predict(out_X_val)\n",
    "                \n",
    "        # Get the accuracy score for the current fold\n",
    "        score = accuracy_score(pred, out_y_val)\n",
    "        \n",
    "        # Append the validation accuracy to outer scores\n",
    "        outer_scores.append(score)\n",
    "        \n",
    "        # Append the best hyperparameters to outer params\n",
    "        outer_params.append(best_params)\n",
    "        out_fold_no += 1\n",
    "    \n",
    "    return outer_scores, outer_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(data, labels, type='euclidean', params=None):\n",
    "    '''\n",
    "    Perform k-fold cross-validation for KNN.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Feature data.\n",
    "    labels : array-like\n",
    "        Target labels.\n",
    "    type : str, default='rbf'\n",
    "        Distance metric type for KNN.\n",
    "    params : dict, optional\n",
    "        Dictionary of hyperparameters.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    avg_train_score : float\n",
    "        Average training accuracy score across all folds.\n",
    "    avg_score : float\n",
    "        Average validation accuracy score across all folds.\n",
    "    model : KNeighborsClassifier object\n",
    "        Trained KNN model on the entire dataset.\n",
    "    '''\n",
    "    \n",
    "    # 3. Configure k-fold cross-validation\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    # 4. Cross-validation loop\n",
    "    fold_no = 1\n",
    "    accuracy_train_per_fold = []  # List to store train accuracy for each fold\n",
    "    accuracy_per_fold = []  # List to store accuracy for each fold\n",
    "    for train_index, val_index in kfold.split(data, labels):\n",
    "        \n",
    "        # Split the dataset into training and validation sets\n",
    "        X_train, X_val = data[train_index], data[val_index]\n",
    "        y_train, y_val = labels[train_index], labels[val_index]\n",
    "\n",
    "        # Create the KNN model with the specified hyperparameters\n",
    "        model = create_KNN(K=params['K'], metric=type, p=params['P'])\n",
    "\n",
    "        # Train the model on the training set\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the validation set\n",
    "        pred = model.predict(X_val)\n",
    "        # Get the accuracy score for the current fold\n",
    "        score = accuracy_score(pred, y_val)\n",
    "\n",
    "        # Predict on the training set\n",
    "        train_pred = model.predict(X_train)\n",
    "        # Get the training accuracy score for the current fold\n",
    "        train_score = accuracy_score(train_pred, y_train)\n",
    "\n",
    "        # Append the validation accuracy to the list\n",
    "        accuracy_per_fold.append(score)\n",
    "        # Append the training accuracy to the list\n",
    "        accuracy_train_per_fold.append(train_score)\n",
    "        fold_no += 1\n",
    "\n",
    "    # Calculate the average validation accuracy score across all folds\n",
    "    avg_score = np.mean(accuracy_per_fold)\n",
    "    # Calculate the average training accuracy score across all folds\n",
    "    avg_train_score = np.mean(accuracy_train_per_fold)\n",
    "\n",
    "    # Split the dataset for final training (80% training, 20% validation)\n",
    "    _, X_val, _, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create the KNN model with the specified hyperparameters\n",
    "    model = create_KNN(K=params['K'], metric=type, p=params['P'])\n",
    "\n",
    "    # Train the model on the entire dataset\n",
    "    model.fit(data, labels)\n",
    "\n",
    "    return avg_train_score, avg_score, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greed search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greed_search(data, labels, type='euclidean', param_grid=None):\n",
    "    '''\n",
    "    Perform greedy search for hyperparameter tuning.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Feature data.\n",
    "    labels : array-like\n",
    "        Target labels.\n",
    "    type : str, default='euclidean'\n",
    "        Distance metric type for KNN.\n",
    "    param_grid : list of dict, optional\n",
    "        List of hyperparameter combinations.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    best_train_scores : list of float\n",
    "        Best training scores obtained during the search.\n",
    "    best_scores : list of float\n",
    "        Best validation scores obtained during the search.\n",
    "    best_params_list : list of dict\n",
    "        Best parameter configurations.\n",
    "    best_models : list of KNeighborsClassifier object\n",
    "        Best models trained with the best parameter configurations.\n",
    "    '''\n",
    "    best_train_scores = []  # List to store the training scores\n",
    "    best_scores = []  # List to store the validation scores\n",
    "    best_params_list = []  # List to store the parameter configurations\n",
    "    best_models = []  # List to store the models\n",
    "    \n",
    "    # Iterate over each combination of hyperparameters in the parameter grid\n",
    "    for params in param_grid:\n",
    "        # Perform k-fold cross-validation with the current set of hyperparameters\n",
    "        train_score, score, model = k_fold_cross_validation(data, labels, type, params=params)\n",
    "\n",
    "        # Add the results to the respective lists\n",
    "        best_train_scores.append(train_score)\n",
    "        best_scores.append(score)\n",
    "        best_params_list.append(params)\n",
    "        best_models.append(model)\n",
    "\n",
    "        # Sort the list of scores in descending order and keep only the top 5\n",
    "        sorted_indices = np.argsort(best_scores)[::-1]  # Get the indices that would sort the scores in descending order\n",
    "        best_train_scores = [best_train_scores[i] for i in sorted_indices][:5]  # Keep the top 5 training scores\n",
    "        best_scores = [best_scores[i] for i in sorted_indices][:5]  # Keep the top 5 validation scores\n",
    "        best_params_list = [best_params_list[i] for i in sorted_indices][:5]  # Keep the top 5 parameter configurations\n",
    "        best_models = [best_models[i] for i in sorted_indices][:5]  # Keep the top 5 models\n",
    "\n",
    "    return best_train_scores, best_scores, best_params_list, best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection(data, labels):\n",
    "    '''\n",
    "    Perform hyperparameter selection for KNN models.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Feature data.\n",
    "    labels : array-like\n",
    "        Target labels.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    best_train_scores : list of float\n",
    "        Best training scores obtained during the search.\n",
    "    best_scores : list of float\n",
    "        Best validation scores obtained during the search.\n",
    "    best_params_list : list of dict\n",
    "        Best parameter configurations.\n",
    "    best_models : list of KNeighborsClassifier object\n",
    "        Best models trained with the best parameter configurations.\n",
    "    '''\n",
    "\n",
    "    # Define the range of hyperparameters\n",
    "    param_ranges = {\n",
    "        \"K\": (1, 75, 1),  # From 1 to 75 with step of 1\n",
    "        \"P\": (3, 10, 1),  # From 3 to 10 with step of 1\n",
    "    }\n",
    "\n",
    "    print(\"Generating hyperparameter combinations...\")\n",
    "    # Generate all combinations of hyperparameters based on the specified ranges\n",
    "    param_grid = generate_hyperparameter_combinations(param_ranges)\n",
    "\n",
    "    best_train_scores = []  # List to store the best training scores\n",
    "    best_scores = []  # List to store the best validation scores\n",
    "    best_params_list = []  # List to store the best parameter configurations\n",
    "    best_models = []  # List to store the best models\n",
    "    best_types = []  # List to store the distance metric types\n",
    "\n",
    "    # Define the distance metric types to be tested\n",
    "    types = ['euclidean', 'manhattan', 'chebyshev', 'minkowski']\n",
    "    for type in types:\n",
    "        # Perform greedy search for each distance metric type\n",
    "        actual_train_scores, actual_scores, actual_params_list, actual_models = greed_search(data, labels, type, param_grid)\n",
    "\n",
    "        # Extend the lists with the results from the current distance metric type\n",
    "        best_train_scores.extend(actual_train_scores)\n",
    "        best_scores.extend(actual_scores)\n",
    "        best_params_list.extend(actual_params_list)\n",
    "        best_models.extend(actual_models)\n",
    "        best_types.extend([type] * len(actual_scores))\n",
    "\n",
    "        # Sort the scores in descending order and keep only the top 5\n",
    "        sorted_indices = np.argsort(best_scores)[::-1]\n",
    "        best_train_scores = [best_train_scores[i] for i in sorted_indices][:5]\n",
    "        best_scores = [best_scores[i] for i in sorted_indices][:5]\n",
    "        best_params_list = [best_params_list[i] for i in sorted_indices][:5]\n",
    "        best_models = [best_models[i] for i in sorted_indices][:5]\n",
    "        best_types = [best_types[i] for i in sorted_indices][:5]\n",
    "\n",
    "    # Print the best scores, distance metric types, and parameter configurations\n",
    "    for i in range(len(best_scores)):\n",
    "        print(f\"Score: {best_scores[i]}, train score: {best_train_scores[i]}, type: {best_types[i]}, parameters: {best_params_list[i]}\")\n",
    "\n",
    "    return best_train_scores, best_scores, best_params_list, best_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------MONK1-----------------\n",
      "Generating hyperparameter combinations...\n",
      "Score: 0.7833333333333334, train score: 1.0, type: manhattan, parameters: {'K': 8, 'P': 5}\n",
      "Score: 0.7833333333333334, train score: 1.0, type: manhattan, parameters: {'K': 8, 'P': 3}\n",
      "Score: 0.7833333333333334, train score: 1.0, type: manhattan, parameters: {'K': 8, 'P': 4}\n",
      "Score: 0.7833333333333334, train score: 1.0, type: manhattan, parameters: {'K': 8, 'P': 6}\n",
      "Score: 0.7833333333333334, train score: 1.0, type: manhattan, parameters: {'K': 8, 'P': 10}\n",
      "-----------------MONK2-----------------\n",
      "Generating hyperparameter combinations...\n",
      "Score: 0.6452205882352942, train score: 1.0, type: euclidean, parameters: {'K': 26, 'P': 10}\n",
      "Score: 0.6452205882352942, train score: 1.0, type: euclidean, parameters: {'K': 26, 'P': 6}\n",
      "Score: 0.6452205882352942, train score: 1.0, type: euclidean, parameters: {'K': 26, 'P': 4}\n",
      "Score: 0.6452205882352942, train score: 1.0, type: euclidean, parameters: {'K': 26, 'P': 3}\n",
      "Score: 0.6452205882352942, train score: 1.0, type: euclidean, parameters: {'K': 26, 'P': 5}\n",
      "-----------------MONK3-----------------\n",
      "Generating hyperparameter combinations...\n",
      "Score: 0.925, train score: 1.0, type: minkowski, parameters: {'K': 57, 'P': 3}\n",
      "Score: 0.925, train score: 1.0, type: manhattan, parameters: {'K': 53, 'P': 5}\n",
      "Score: 0.925, train score: 1.0, type: manhattan, parameters: {'K': 53, 'P': 3}\n",
      "Score: 0.925, train score: 1.0, type: manhattan, parameters: {'K': 53, 'P': 4}\n",
      "Score: 0.925, train score: 1.0, type: manhattan, parameters: {'K': 53, 'P': 6}\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------MONK1-----------------\")\n",
    "# Perform hyperparameter selection for the MONK1 dataset and get the best models\n",
    "_, _, best_param_list_monk1, best_models_monk_1 = selection(monks_train[0][0], monks_train[0][1])\n",
    "\n",
    "print(\"-----------------MONK2-----------------\")\n",
    "# Perform hyperparameter selection for the MONK2 dataset and get the best models\n",
    "_, _, best_param_list_monk2, best_models_monk_2 = selection(monks_train[1][0], monks_train[1][1])\n",
    "\n",
    "print(\"-----------------MONK3-----------------\")\n",
    "# Perform hyperparameter selection for the MONK3 dataset and get the best models\n",
    "_, _, best_param_list_monk3, best_models_monk_3 = selection(monks_train[2][0], monks_train[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MONK 1: 0.734\n",
      "Variance MONK 1: 0.006064\n"
     ]
    }
   ],
   "source": [
    "# Generate all combinations of hyperparameters based on the specified ranges\n",
    "# We are using the best hyperparameters found for the MONK1 dataset\n",
    "param_grid_final_monk1 = [best_param_list_monk1[0]]\n",
    "\n",
    "# Perform double k-fold cross-validation on the MONK1 dataset\n",
    "# Using the 'manhattan' distance metric, 5 outer folds, and 5 inner folds\n",
    "scores_monk1, _ = double_k_fold_cross_validation(\n",
    "    monks_train[0][0],  # Features of the MONK1 training dataset\n",
    "    monks_train[0][1],  # Labels of the MONK1 training dataset\n",
    "    type='manhattan',   # Distance metric type\n",
    "    outer_k=5,          # Number of outer folds\n",
    "    inner_k=5,          # Number of inner folds\n",
    "    param_grid=param_grid_final_monk1  # Hyperparameter grid\n",
    ")\n",
    "\n",
    "# Calculate the variance of the scores obtained from the cross-validation\n",
    "variance_1 = np.var(scores_monk1)\n",
    "\n",
    "# Calculate the mean of the scores obtained from the cross-validation\n",
    "mean_monk1 = np.mean(scores_monk1)\n",
    "\n",
    "# Print the mean and variance of the scores\n",
    "print(f\"Mean MONK 1: {mean_monk1}\")\n",
    "print(f\"Variance MONK 1: {variance_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MONK 2: 0.6151515151515151\n",
      "Variance MONK 2: 0.0005177284007104704\n"
     ]
    }
   ],
   "source": [
    "# Generate all combinations of hyperparameters based on the specified ranges\n",
    "# We are using the second best hyperparameters found for the MONK1 dataset\n",
    "param_grid_final_monk2 = [best_param_list_monk1[1]]\n",
    "\n",
    "# Perform double k-fold cross-validation on the MONK2 dataset\n",
    "# Using the 'euclidean' distance metric, 5 outer folds, and 5 inner folds\n",
    "scores_monk2, _ = double_k_fold_cross_validation(\n",
    "    monks_train[1][0],  # Features of the MONK2 training dataset\n",
    "    monks_train[1][1],  # Labels of the MONK2 training dataset\n",
    "    type='euclidean',   # Distance metric type\n",
    "    outer_k=5,          # Number of outer folds\n",
    "    inner_k=5,          # Number of inner folds\n",
    "    param_grid=param_grid_final_monk2  # Hyperparameter grid\n",
    ")\n",
    "\n",
    "# Calculate the variance of the scores obtained from the cross-validation\n",
    "variance_2 = np.var(scores_monk2)\n",
    "\n",
    "# Calculate the mean of the scores obtained from the cross-validation\n",
    "mean_monk2 = np.mean(scores_monk2)\n",
    "\n",
    "# Print the mean and variance of the scores\n",
    "print(f\"Mean MONK 2: {mean_monk2}\")\n",
    "print(f\"Variance MONK 2: {variance_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MONK 3: 0.877\n",
      "Variance MONK 3: 0.00278377777777778\n"
     ]
    }
   ],
   "source": [
    "# Generate all combinations of hyperparameters based on the specified ranges\n",
    "# We are using the third best hyperparameters found for the MONK3 dataset\n",
    "param_grid_final_monk3 = [best_param_list_monk3[2]]\n",
    "\n",
    "# Perform double k-fold cross-validation on the MONK3 dataset\n",
    "# Using the 'minkowski' distance metric, 5 outer folds, and 5 inner folds\n",
    "scores_monk3, _ = double_k_fold_cross_validation(\n",
    "    monks_train[2][0],  # Features of the MONK3 training dataset\n",
    "    monks_train[2][1],  # Labels of the MONK3 training dataset\n",
    "    type='minkowski',   # Distance metric type\n",
    "    outer_k=5,          # Number of outer folds\n",
    "    inner_k=5,          # Number of inner folds\n",
    "    param_grid=param_grid_final_monk3  # Hyperparameter grid\n",
    ")\n",
    "\n",
    "# Calculate the variance of the scores obtained from the cross-validation\n",
    "variance_3 = np.var(scores_monk3)\n",
    "\n",
    "# Calculate the mean of the scores obtained from the cross-validation\n",
    "mean_monk3 = np.mean(scores_monk3)\n",
    "\n",
    "# Print the mean and variance of the scores\n",
    "print(f\"Mean MONK 3: {mean_monk3}\")\n",
    "print(f\"Variance MONK 3: {variance_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8402777777777778\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.90      0.85       216\n",
      "           1       0.88      0.78      0.83       216\n",
      "\n",
      "    accuracy                           0.84       432\n",
      "   macro avg       0.84      0.84      0.84       432\n",
      "weighted avg       0.84      0.84      0.84       432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation for MONK1 dataset\n",
    "\n",
    "# Predict the labels for the test set using the best model found for MONK1\n",
    "y1_pred = best_models_monk_1[0].predict(X1_test_encoded)\n",
    "\n",
    "# Print the accuracy of the model on the test set\n",
    "print(\"Accuracy:\", accuracy_score(y1_test, y1_pred))\n",
    "\n",
    "# Print the detailed classification report which includes precision, recall, f1-score, and support\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y1_test, y1_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7939814814814815\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.93      0.86       290\n",
      "           1       0.78      0.51      0.62       142\n",
      "\n",
      "    accuracy                           0.79       432\n",
      "   macro avg       0.79      0.72      0.74       432\n",
      "weighted avg       0.79      0.79      0.78       432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation for MONK2 dataset\n",
    "\n",
    "# Predict the labels for the test set using the best model found for MONK2\n",
    "y2_pred = best_models_monk_2[0].predict(X2_test_encoded)\n",
    "\n",
    "# Print the accuracy of the model on the test set\n",
    "print(\"Accuracy:\", accuracy_score(y2_test, y2_pred))\n",
    "\n",
    "# Print the detailed classification report which includes precision, recall, f1-score, and support\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y2_test, y2_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9305555555555556\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93       204\n",
      "           1       0.98      0.89      0.93       228\n",
      "\n",
      "    accuracy                           0.93       432\n",
      "   macro avg       0.93      0.93      0.93       432\n",
      "weighted avg       0.93      0.93      0.93       432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set for MONK3\n",
    "# Use the best model found for the MONK3 dataset to predict the labels of the test set\n",
    "y3_pred = best_models_monk_3[0].predict(X3_test_encoded)\n",
    "\n",
    "# Report the results\n",
    "# Print the accuracy of the model on the test set\n",
    "print(\"Accuracy:\", accuracy_score(y3_test, y3_pred))\n",
    "\n",
    "# Print the detailed classification report which includes precision, recall, f1-score, and support\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y3_test, y3_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
