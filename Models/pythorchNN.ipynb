{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN implemetation with Pythorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.Y = torch.tensor(Y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per normalizzare i dati\n",
    "def normalize(data):\n",
    "    scaler = StandardScaler()\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, labels, k=5):\n",
    "    \"\"\"\n",
    "    Divide i dati in k fold.\n",
    "    \n",
    "    Args:\n",
    "        data (np.ndarray | pd.DataFrame): Dati di input.\n",
    "        labels (np.ndarray | pd.Series): Etichette.\n",
    "        k (int): Numero di fold.\n",
    "    \n",
    "    Returns:\n",
    "        list: Lista di tuple (fold_data, fold_labels).\n",
    "    \"\"\"\n",
    "    if not isinstance(data, np.ndarray):\n",
    "        data = data.to_numpy()  # Converte DataFrame in NumPy array\n",
    "    if not isinstance(labels, np.ndarray):\n",
    "        labels = labels.to_numpy()  # Converte Series in NumPy array\n",
    "    \n",
    "    data = normalize(data)\n",
    "\n",
    "    fold_size = len(data) // k\n",
    "    indices = np.arange(len(data))\n",
    "    np.random.shuffle(indices)\n",
    "    folds = []\n",
    "\n",
    "    for i in range(k):\n",
    "        fold_indices = indices[i * fold_size: (i + 1) * fold_size]\n",
    "        fold_data = data[fold_indices]\n",
    "        fold_labels = labels[fold_indices]\n",
    "        folds.append((fold_data, fold_labels))\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hyperparameter_combinations(param_ranges):\n",
    "    \"\"\"\n",
    "    Genera tutte le combinazioni di iperparametri basate su range e step specificati.\n",
    "\n",
    "    :param param_ranges: Dizionario con i nomi degli iperparametri come chiavi.\n",
    "                         Ogni valore Ã¨ una tupla (start, stop, step).\n",
    "    :return: Lista di dizionari con tutte le combinazioni possibili.\n",
    "    \"\"\"\n",
    "    param_values = {\n",
    "        key: np.arange(start, stop + step, step)\n",
    "        for key, (start, stop, step) in param_ranges.items()\n",
    "    }\n",
    "    \n",
    "    param_combinations = list(itertools.product(*param_values.values()))\n",
    "    return [\n",
    "        dict(zip(param_values.keys(), combination))\n",
    "        for combination in param_combinations\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation, train and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, hidden_layers, alpha):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc_input = nn.Linear(input_size, hidden_size)\n",
    "        self.fc_hidden = [None] * (hidden_layers-1)\n",
    "        for i in range(hidden_layers-1):\n",
    "            self.fc_hidden[i] = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_output = nn.Linear(hidden_size, output_size)\n",
    "        self.leacky_relu = nn.LeakyReLU(alpha)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.leacky_relu(self.fc_input(x))\n",
    "        for i in range(len(self.fc_hidden)):\n",
    "            x = self.leacky_relu(self.fc_hidden[i](x))\n",
    "        x = self.sigmoid(self.fc_output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(data_loader, model, learning_rate, momentum, weight_decay, epochs, val_data=[], val_labels=[]):\n",
    "    criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(), \n",
    "        lr=learning_rate, \n",
    "        momentum=momentum, \n",
    "        nesterov=True, \n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    history = {'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
    "    # Addestramento\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        correct_predictions = 0  \n",
    "        total_samples = 0  \n",
    "        last_loss = 0 \n",
    "        for inputs, labels in data_loader:\n",
    "            optimizer.zero_grad()  # Reset dei gradienti\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))  # Calcolo della perdita\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Aggiornamento dei pesi\n",
    "\n",
    "            last_loss = loss.item()  # Memorizziamo l'ultima perdita\n",
    "\n",
    "            # Calcolo dell'accuratezza\n",
    "            predicted = (outputs >= 0.5).float()  # La soglia 0.5 per classificazione binaria\n",
    "            correct_predictions += (predicted == labels.unsqueeze(1)).sum().item()  # Somma delle previsioni corrette\n",
    "            total_samples += labels.size(0)  # Numero totale di esempi nel batch\n",
    "\n",
    "\n",
    "        # Calcolo dell'accuratezza del train per epoca\n",
    "        accuracy = 100 * correct_predictions / total_samples\n",
    "        history['train_loss'].append(last_loss)\n",
    "        history['train_accuracy'].append(accuracy)\n",
    "\n",
    "        #validazione di quell'epoca\n",
    "        if val_data is not None and val_labels is not None and len(val_data) > 0 and len(val_labels) > 0:\n",
    "            result = evaluation(model, val_data, val_labels, criterion)\n",
    "            history['val_loss'].append(result[0])\n",
    "            history['val_accuracy'].append(result[1])\n",
    "    return history\n",
    "\n",
    "def evaluation(model, X, Y, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = torch.tensor(X, dtype=torch.float32)\n",
    "        labels = torch.tensor(Y, dtype=torch.float32)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "        predictions = (outputs.squeeze() > 0.5).float()  # Soglia 0.5 per classificazione\n",
    "        accuracy = (predictions == labels.unsqueeze(1)).float().mean()\n",
    "        return [loss,accuracy]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double-k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_k_fold_cross_validation(data, labels, in_size = 6, out_size = 1, outer_k = 5, inner_k = 5, param_grid=None):\n",
    "    \"\"\"\n",
    "    Implementa una Double K-Fold Cross-Validation\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Caratteristiche del dataset.\n",
    "        labels (np.ndarray): Etichette del dataset.\n",
    "        outer_k (int): Numero di fold per la validazione esterna.\n",
    "        inner_k (int): Numero di fold per l'ottimizzazione iperparametri.\n",
    "        param_grid (list): Lista di dizionari con gli iperparametri da provare.\n",
    "    \n",
    "    Returns:\n",
    "        list: Lista dei punteggi ottenuti per ogni fold esterno.\n",
    "    \"\"\"\n",
    "    outer_scores = []\n",
    "    outer_params = []\n",
    "    outer_folds = split_data(data, labels, k=outer_k)\n",
    "    \n",
    "    for i in range(outer_k):\n",
    "        print(\"Outer fold\", i + 1)\n",
    "        outer_test_data, outer_test_labels = outer_folds[i]\n",
    "        outer_train_data = np.concatenate([fold[0] for j, fold in enumerate(outer_folds) if j != i])\n",
    "        outer_train_labels = np.concatenate([fold[1] for j, fold in enumerate(outer_folds) if j != i])\n",
    "        \n",
    "        best_params = {}\n",
    "        best_score = -np.inf\n",
    "        \n",
    "        # Validazione interna per ottimizzazione iperparametri\n",
    "        inner_folds = split_data(outer_train_data, outer_train_labels, k=inner_k)\n",
    "\n",
    "        for params in param_grid:\n",
    "            inner_scores = []\n",
    "            \n",
    "            for j in range(inner_k):\n",
    "                inner_val_data, inner_val_labels = inner_folds[j]\n",
    "                inner_train_data = np.concatenate([fold[0] for l, fold in enumerate(inner_folds) if l != j])\n",
    "                inner_train_labels = np.concatenate([fold[1] for l, fold in enumerate(inner_folds) if l != j])\n",
    "                dataset = CustomDataset(inner_train_data, inner_train_labels)\n",
    "                data_loader = DataLoader(dataset, batch_size=int(params['batch_size']), shuffle=False)\n",
    "                # Creazione della rete neurale\n",
    "                model = NN(input_size = in_size, hidden_size = params['hidden_size'], output_size = out_size, \n",
    "                           hidden_layers = params['hidden_layers'], alpha=params['alpha'])\n",
    "                # Allena il modello e ottieni la cronologia (history)\n",
    "                history = fit(data_loader=data_loader, model=model, learning_rate=params['learning_rate'],\n",
    "                              weight_decay=params['regularization'], momentum=params['momentum'],\n",
    "                              epochs=params['epochs'], val_data=inner_val_data, val_labels=inner_val_labels)\n",
    "    \n",
    "                inner_scores.append(max(history['val_accuracy'])\n",
    ")\n",
    "            \n",
    "            avg_score = np.mean(inner_scores)\n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                best_params = params\n",
    "        \n",
    "        # Addestramento finale sul set di train esterno\n",
    "        dataset = CustomDataset(outer_train_data, outer_train_labels)\n",
    "        data_loader = DataLoader(dataset, batch_size=int(params['batch_size']), shuffle=False)\n",
    "        final_model = NN(input_size = in_size, hidden_size = params['hidden_size'], output_size = out_size, \n",
    "                         hidden_layers = params['hidden_layers'], alpha=params['alpha'])\n",
    "        history = fit(data_loader=data_loader, model=final_model, learning_rate=params['learning_rate'],\n",
    "                      weight_decay=params['regularization'], momentum=params['momentum'],\n",
    "                      epochs=params['epochs'], val_data=outer_test_data, val_labels=outer_test_labels)\n",
    "        outer_scores.append(max(history['val_accuracy']))\n",
    "        outer_params.append(best_params)\n",
    "    \n",
    "    return outer_scores, outer_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(data, labels, in_size, out_size, params, k=5):\n",
    "    \"\"\"\n",
    "    Implementa una K-Fold Cross-Validation\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    folds = split_data(data, labels, input_size = 6, output_size = 1, k=k)\n",
    "\n",
    "\n",
    "    for i in range(k):\n",
    "        # Creazione della rete neurale\n",
    "        model = NN(input_dim = in_size, hidden_size = params['hidden_size'], output_dim = out_size,\n",
    "                   hidden_layers = params['hidden_layers'], alpha=params['alpha'])\n",
    "        test_data, test_labels = folds[i]\n",
    "        train_data = np.concatenate([fold[0] for j, fold in enumerate(folds) if j != i])\n",
    "        train_labels = np.concatenate([fold[1] for j, fold in enumerate(folds) if j != i])\n",
    "        dataset = CustomDataset(train_data, train_labels)\n",
    "        data_loader = DataLoader(dataset, batch_size=int(params['batch_size']), shuffle=False)\n",
    "        history = fit(data_loader=data_loader, model=model, learning_rate=params['learning_rate'], \n",
    "                      weight_decay=params['regularization'], momentum=params['momentum'],\n",
    "                      epochs=params['epochs'], val_data=test_data, val_labels=test_labels)\n",
    "        score = max(history['val_accuracy'])\n",
    "        scores.append(score)\n",
    "            \n",
    "    avg_score = np.mean(scores)\n",
    "\n",
    "    train_data = np.concatenate([fold[0] for fold in folds])\n",
    "    train_labels = np.concatenate([fold[1] for fold in folds])\n",
    "    dataset = CustomDataset(train_data, train_labels)\n",
    "    data_loader = DataLoader(dataset, batch_size=int(params['batch_size']), shuffle=False)\n",
    "    model = NN(input_size = in_size, hidden_size = params['hidden_size'], output_size = out_size,\n",
    "               hidden_layers = params['hidden_layers'], alpha=params['alpha'])\n",
    "        \n",
    "    history = fit(data_loader=data_loader, model=model, learning_rate=params['learning_rate'], \n",
    "                  weight_decay=params['regularization'], momentum=params['momentum'],\n",
    "                  epochs=params['epochs'])\n",
    "\n",
    "\n",
    "    return avg_score, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica i file di addestramento e test per ciascun dataset dal percorso specificato\n",
    "monk1_train = pd.read_csv('../Datasets/Monks/monks-1.train', sep='\\s+', header=None)\n",
    "monk1_test = pd.read_csv('../Datasets/Monks/monks-1.test', sep='\\s+', header=None)\n",
    "\n",
    "monk2_train = pd.read_csv('../Datasets/Monks/monks-2.train', sep='\\s+', header=None)\n",
    "monk2_test = pd.read_csv('../Datasets/Monks/monks-2.test', sep='\\s+', header=None)\n",
    "\n",
    "monk3_train = pd.read_csv('../Datasets/Monks/monks-3.train', sep='\\s+', header=None)\n",
    "monk3_test = pd.read_csv('../Datasets/Monks/monks-3.test', sep='\\s+', header=None)\n",
    "\n",
    "\n",
    "# Separazione tra features e labels per monk1\n",
    "X1_train = monk1_train.iloc[:, 1:7].values  # Caratteristiche (features)\n",
    "y1_train = monk1_train.iloc[:, 0].values   # Etichette (labels)\n",
    "\n",
    "X1_test = monk1_test.iloc[:, 1:7].values\n",
    "y1_test = monk1_test.iloc[:, 0].values\n",
    "\n",
    "# Separazione tra features e labels per monk2\n",
    "X2_train = monk2_train.iloc[:, 1:7].values\n",
    "y2_train = monk2_train.iloc[:, 0].values\n",
    "\n",
    "X2_test = monk2_test.iloc[:, 1:7].values\n",
    "y2_test = monk2_test.iloc[:, 0].values\n",
    "\n",
    "# Separazione tra features e labels per monk3\n",
    "X3_train = monk3_train.iloc[:, 1:7].values\n",
    "y3_train = monk3_train.iloc[:, 0].values\n",
    "\n",
    "X3_test = monk3_test.iloc[:, 1:7].values\n",
    "y3_test = monk3_test.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generazione delle combinazioni di iperparametri...\n",
      "Outer fold 1\n",
      "Outer fold 2\n",
      "Outer fold 3\n",
      "Outer fold 4\n",
      "Outer fold 5\n",
      "Fold 1: 0.4444444477558136\n",
      "  params: {'learning_rate': 0.001, 'epochs': 50, 'batch_size': 8, 'hidden_size': 3, 'hidden_layers': 1, 'momentum': 0.9, 'regularization': 0.01, 'alpha': 0.01}\n",
      "Fold 2: 0.4722222089767456\n",
      "  params: {'learning_rate': 0.001, 'epochs': 50, 'batch_size': 8, 'hidden_size': 3, 'hidden_layers': 1, 'momentum': 0.92, 'regularization': 0.01, 'alpha': 0.01}\n",
      "Fold 3: 0.4652777910232544\n",
      "  params: {'learning_rate': 0.001, 'epochs': 50, 'batch_size': 8, 'hidden_size': 3, 'hidden_layers': 1, 'momentum': 0.91, 'regularization': 0.01, 'alpha': 0.01}\n",
      "Fold 4: 0.5243055820465088\n",
      "  params: {'learning_rate': 0.001, 'epochs': 50, 'batch_size': 8, 'hidden_size': 3, 'hidden_layers': 1, 'momentum': 0.92, 'regularization': 0.01, 'alpha': 0.01}\n",
      "Fold 5: 0.4652777910232544\n",
      "  params: {'learning_rate': 0.001, 'epochs': 50, 'batch_size': 8, 'hidden_size': 3, 'hidden_layers': 1, 'momentum': 0.92, 'regularization': 0.01, 'alpha': 0.01}\n",
      "Punteggio medio: 0.47430554\n"
     ]
    }
   ],
   "source": [
    "# Definisce i parametri della rete neurale\n",
    "input_size = 6\n",
    "output_size = 1\n",
    "\n",
    "# Definizione dei range degli iperparametri\n",
    "param_ranges = {\n",
    "    \"learning_rate\": (0.001, 0.001, 0.005),  # Da 0.001 a 0.01 con step di 0.005\n",
    "    \"epochs\": (50, 50, 1),                   # Da 0 a 10 con step di 1\n",
    "    \"batch_size\": (8, 8, 8),             # Da 8 a 32 con step di 8\n",
    "    \"hidden_size\": (3, 3, 1),           # Da 32 a 128 con step di 32\n",
    "    \"hidden_layers\": (1, 1, 1),             # Da 1 a 3 con step di 1\n",
    "    \"momentum\": (0.9, 0.92, 0.01),          # Da 0.9 a 0.99 con step di 0.01\n",
    "    \"regularization\": (0.01, 0.01, 0.05),             # Da 0.0 a 0.1 con step di 0.05\n",
    "    \"alpha\": (0.01, 0.01, 0.01)              # Da 0.01 a 0.1 con step di 0.05\n",
    "}\n",
    "\n",
    "#start_time = time.time()\n",
    "print(\"Generazione delle combinazioni di iperparametri...\")\n",
    "param_grid = []\n",
    "param_grid = generate_hyperparameter_combinations(param_ranges)\n",
    "#end_time = time.time()\n",
    "\n",
    "#execution_time = end_time - start_time\n",
    "#print(f\"Tempo di esecuzione di generate_hyperparameter_combinations: {execution_time} secondi\")\n",
    "\n",
    "scores,params = double_k_fold_cross_validation(X1_train, y1_train, input_size, output_size, outer_k=5, inner_k=3, param_grid=param_grid)\n",
    "for i in range(len(scores)):\n",
    "    print(f\"Fold {i + 1}: {scores[i]}\")\n",
    "    print(f\"  params: {params[i]}\")\n",
    "print(\"Punteggio medio:\", np.mean(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
