{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN implemetation with Pythorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.Y = torch.tensor(Y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0, path='checkpoint.pt', verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Numero di epoche da aspettare prima di interrompere l'allenamento\n",
    "                            se non c'è miglioramento. Default: 5.\n",
    "            delta (float): Miglioramento minimo richiesto per considerare un progresso. Default: 0.\n",
    "            path (str): Percorso dove salvare il modello migliore. Default: 'checkpoint.pt'.\n",
    "            verbose (bool): Stampa messaggi quando il modello migliora. Default: False.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping: No improvement for {self.counter} epochs\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Salva il modello quando la perdita di validazione migliora.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...\")\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per normalizzare i dati\n",
    "def normalize(data):\n",
    "    scaler = StandardScaler()\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, labels, k=5):\n",
    "    \"\"\"\n",
    "    Divide i dati in k fold.\n",
    "    \n",
    "    Args:\n",
    "        data (np.ndarray | pd.DataFrame): Dati di input.\n",
    "        labels (np.ndarray | pd.Series): Etichette.\n",
    "        k (int): Numero di fold.\n",
    "    \n",
    "    Returns:\n",
    "        list: Lista di tuple (fold_data, fold_labels).\n",
    "    \"\"\"\n",
    "    if not isinstance(data, np.ndarray):\n",
    "        data = data.to_numpy()  # Converte DataFrame in NumPy array\n",
    "    if not isinstance(labels, np.ndarray):\n",
    "        labels = labels.to_numpy()  # Converte Series in NumPy array\n",
    "    \n",
    "    data = normalize(data)\n",
    "\n",
    "    fold_size = len(data) // k\n",
    "    indices = np.arange(len(data))\n",
    "    np.random.shuffle(indices)\n",
    "    folds = []\n",
    "\n",
    "    for i in range(k):\n",
    "        fold_indices = indices[i * fold_size: (i + 1) * fold_size]\n",
    "        fold_data = data[fold_indices]\n",
    "        fold_labels = labels[fold_indices]\n",
    "        folds.append((fold_data, fold_labels))\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss and accuracy from a model's history, \n",
    "    adding the parameters as a title.\n",
    "\n",
    "    Parameters:\n",
    "    - history: History object returned by model.fit().\n",
    "    - params: Dictionary containing the model parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Estrai dati dalla history\n",
    "    train_loss = history['train_loss']\n",
    "    val_loss = history['val_loss']\n",
    "    train_acc = history['train_accuracy']\n",
    "    val_acc = history['val_accuracy']\n",
    "    \n",
    "    \n",
    "    epochs = range(1, len(train_loss) + 1)  # Numero di epoche\n",
    "\n",
    "    # Grafico della Loss\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss, 'b-o', label='Training Loss')\n",
    "    if val_loss:\n",
    "        plt.plot(epochs, val_loss, 'r-o', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Grafico dell'Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    if train_acc:\n",
    "        plt.plot(epochs, train_acc, 'b-o', label='Training Accuracy')\n",
    "    if val_acc:\n",
    "        plt.plot(epochs, val_acc, 'r-o', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hyperparameter_combinations(param_ranges):\n",
    "    \"\"\"\n",
    "    Genera tutte le combinazioni di iperparametri basate su range e step specificati.\n",
    "\n",
    "    :param param_ranges: Dizionario con i nomi degli iperparametri come chiavi.\n",
    "                         Ogni valore è una tupla (start, stop, step).\n",
    "    :return: Lista di dizionari con tutte le combinazioni possibili.\n",
    "    \"\"\"\n",
    "    param_values = {\n",
    "        key: np.arange(start, stop + step, step)\n",
    "        for key, (start, stop, step) in param_ranges.items()\n",
    "    }\n",
    "    \n",
    "    param_combinations = list(itertools.product(*param_values.values()))\n",
    "    return [\n",
    "        dict(zip(param_values.keys(), combination))\n",
    "        for combination in param_combinations\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation, train and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, hidden_layers, alpha, activ_type = 'tanh'):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc_input = nn.Linear(input_size, hidden_size)\n",
    "        self.fc_hidden = [None] * (hidden_layers-1)\n",
    "        for i in range(hidden_layers-1):\n",
    "            self.fc_hidden[i] = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_output = nn.Linear(hidden_size, output_size)\n",
    "        if activ_type.startswith(\"t\"):\n",
    "            self.activ_f = nn.Tanh()\n",
    "        if activ_type.startswith(\"l\"):\n",
    "            self.activ_f = nn.LeakyReLU(alpha)\n",
    "        if activ_type.startswith(\"r\"):\n",
    "            self.activ_f = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.activ_f(self.fc_input(x))\n",
    "        for i in range(len(self.fc_hidden)):\n",
    "            x = self.activ_f(self.fc_hidden[i](x))\n",
    "        x = self.sigmoid(self.fc_output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(data_loader, model, learning_rate, momentum, weight_decay, epochs, patience, optim_type = 'SGD', reg_flag = False, val_data=[], val_labels=[]):\n",
    "    criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "    if reg_flag:\n",
    "        if optim_type == 'Adam':\n",
    "            optimizer = optim.Adam(\n",
    "            model.parameters(), \n",
    "            lr=learning_rate, \n",
    "            momentum=momentum, \n",
    "            nesterov=True, \n",
    "            weight_decay=weight_decay\n",
    "            )\n",
    "        else:\n",
    "            optimizer = optim.SGD(\n",
    "            model.parameters(), \n",
    "            lr=learning_rate, \n",
    "            momentum=momentum, \n",
    "            nesterov=True, \n",
    "            weight_decay=weight_decay\n",
    "            )\n",
    "    else:\n",
    "        if optim_type == 'Adam':\n",
    "            optimizer = optim.Adam(\n",
    "            model.parameters(), \n",
    "            lr=learning_rate, \n",
    "            momentum=momentum, \n",
    "            nesterov=True \n",
    "            )\n",
    "        else:\n",
    "            optimizer = optim.SGD(\n",
    "            model.parameters(), \n",
    "            lr=learning_rate, \n",
    "            momentum=momentum, \n",
    "            nesterov=True \n",
    "            )\n",
    "\n",
    "    history = {'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
    "    early_stopping = EarlyStopping(patience, verbose=True)\n",
    "    # Addestramento\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        correct_predictions = 0  \n",
    "        total_samples = 0  \n",
    "        last_loss = 0 \n",
    "        for inputs, labels in data_loader:\n",
    "            optimizer.zero_grad()  # Reset dei gradienti\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))  # Calcolo della perdita\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Aggiornamento dei pesi\n",
    "\n",
    "            last_loss = loss.item()  # Memorizziamo l'ultima perdita\n",
    "\n",
    "            # Calcolo dell'accuratezza\n",
    "            predicted = (outputs >= 0.5).float()  # La soglia 0.5 per classificazione binaria\n",
    "            correct_predictions += (predicted == labels.unsqueeze(1)).sum().item()  # Somma delle previsioni corrette\n",
    "            total_samples += labels.size(0)  # Numero totale di esempi nel batch\n",
    "\n",
    "\n",
    "        # Calcolo dell'accuratezza del train per epoca\n",
    "        accuracy = 100 * correct_predictions / total_samples\n",
    "        history['train_loss'].append(last_loss)\n",
    "        history['train_accuracy'].append(accuracy)\n",
    "\n",
    "    \n",
    "        #validazione di quell'epoca\n",
    "        if val_data is not None and val_labels is not None and len(val_data) > 0 and len(val_labels) > 0:\n",
    "            result = evaluation(model, val_data, val_labels, criterion)\n",
    "            history['val_loss'].append(result[0])\n",
    "            history['val_accuracy'].append(result[1])\n",
    "            early_stopping(result[0], model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break\n",
    "            \n",
    "    return history\n",
    "\n",
    "def evaluation(model, X, Y, criterion):\n",
    "    correct_predictions = 0  \n",
    "    total_samples = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = torch.tensor(X, dtype=torch.float32)\n",
    "        labels = torch.tensor(Y, dtype=torch.float32)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "        predicted = (outputs >= 0.5).float()  # La soglia 0.5 per classificazione binaria\n",
    "        correct_predictions += (predicted == labels.unsqueeze(1)).sum().item()  # Somma delle previsioni corrette\n",
    "        total_samples += labels.size(0)  # Numero totale di esempi nel batc\n",
    "        accuracy = 100 * correct_predictions / total_samples\n",
    "        print(accuracy)\n",
    "        return [loss.item(),accuracy]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double-k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_k_fold_cross_validation(data, labels, activ_type = 'tanh', optim_type = 'SGD',\n",
    "                                   reg_flag = False, in_size = 6, out_size = 1, outer_k = 5, inner_k = 5, param_grid=None):\n",
    "    \"\"\"\n",
    "    Implementa una Double K-Fold Cross-Validation\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Caratteristiche del dataset.\n",
    "        labels (np.ndarray): Etichette del dataset.\n",
    "        outer_k (int): Numero di fold per la validazione esterna.\n",
    "        inner_k (int): Numero di fold per l'ottimizzazione iperparametri.\n",
    "        param_grid (list): Lista di dizionari con gli iperparametri da provare.\n",
    "    \n",
    "    Returns:\n",
    "        list: Lista dei punteggi ottenuti per ogni fold esterno.\n",
    "    \"\"\"\n",
    "    outer_scores = []\n",
    "    outer_params = []\n",
    "    # Configurazione della k-fold cross-validation\n",
    "    out_kfold = StratifiedKFold(n_splits=outer_k, shuffle=True, random_state=42)\n",
    "\n",
    "    # 4. Ciclo di cross-validation\n",
    "    out_fold_no = 1\n",
    "    for train_index, val_index in out_kfold.split(data, labels):\n",
    "        \n",
    "        # Suddivisione del dataset\n",
    "        out_X_train, out_X_val = data[train_index], data[val_index]\n",
    "        out_y_train, out_y_val = labels[train_index], labels[val_index]\n",
    "        \n",
    "        best_params = {}\n",
    "        best_score = -np.inf\n",
    "\n",
    "        for params in param_grid:\n",
    "\n",
    "            inner_scores = []\n",
    "\n",
    "            # 4. Ciclo di cross-validation\n",
    "            inner_fold_no = 1\n",
    "            inner_kfold = StratifiedKFold(n_splits=inner_k, shuffle=True, random_state=42)\n",
    "\n",
    "            for train_index, val_index in inner_kfold.split(out_X_train, out_y_train):\n",
    "                \n",
    "                # Suddivisione del dataset\n",
    "                inner_X_train, inner_X_val = out_X_train[train_index], out_X_train[val_index]\n",
    "                inner_y_train, inner_y_val = out_y_train[train_index], out_y_train[val_index]\n",
    "\n",
    "                dataset = CustomDataset(inner_X_train, inner_y_train)\n",
    "                data_loader = DataLoader(dataset, batch_size=int(params['batch_size']), shuffle=False)\n",
    "\n",
    "                # Creazione della rete neurale\n",
    "                model = NN(input_size = in_size, hidden_size = params['hidden_size'], output_size = out_size, \n",
    "                           hidden_layers = params['hidden_layers'], alpha=params['alpha'])\n",
    "                # Allena il modello e ottieni la cronologia (history)\n",
    "                history = fit(data_loader=data_loader, model=model, learning_rate=params['learning_rate'],\n",
    "                              weight_decay=params['regularization'], momentum=params['momentum'],\n",
    "                              epochs=params['epochs'], val_data=inner_X_val, val_labels=inner_y_val)\n",
    "    \n",
    "                inner_scores.append(max(history['val_accuracy']))\n",
    "                inner_fold_no += 1\n",
    "            \n",
    "            avg_score = np.mean(inner_scores)\n",
    "\n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                best_params = params\n",
    "        \n",
    "\n",
    "         # Addestramento finale sul set di train esterno\n",
    "        dataset = CustomDataset(out_X_train, out_y_train)\n",
    "        data_loader = DataLoader(dataset, batch_size=int(params['batch_size']), shuffle=False)\n",
    "\n",
    "        final_model = NN(input_size = in_size, hidden_size = params['hidden_size'], output_size = out_size, \n",
    "                         hidden_layers = params['hidden_layers'], alpha=params['alpha'])\n",
    "        \n",
    "        history = fit(data_loader=data_loader, model=final_model, learning_rate=params['learning_rate'],\n",
    "                      weight_decay=params['regularization'], momentum=params['momentum'],\n",
    "                      epochs=params['epochs'], val_data=out_X_val, val_labels=out_y_val)\n",
    "        outer_scores.append(max(history['val_accuracy']))\n",
    "        outer_params.append(best_params)\n",
    "\n",
    "        out_fold_no += 1\n",
    "    \n",
    "    return outer_scores, outer_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(data, labels, activ_type = 'tanh', optim_type = 'SGD', reg_flag = False, input_size =6, output_size = 1, params=None):\n",
    "    # Configurazione della k-fold cross-validation\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # 4. Ciclo di cross-validation\n",
    "    fold_no = 1\n",
    "    accuracy_per_fold = []\n",
    "    for train_index, val_index in kfold.split(data, labels):\n",
    "        \n",
    "        # Suddivisione del dataset\n",
    "        X_train, X_val = data[train_index], data[val_index]\n",
    "        y_train, y_val = labels[train_index], labels[val_index]\n",
    "        \n",
    "\n",
    "        dataset = CustomDataset(X_train, y_train)\n",
    "        data_loader = DataLoader(dataset, batch_size=int(params['batch_size']), shuffle=False)\n",
    "\n",
    "        # Creazione della rete neurale\n",
    "        model = NN(input_size = input_size,\n",
    "                    hidden_size = params['hidden_size'],\n",
    "                    output_size = output_size,\n",
    "                    hidden_layers = params['hidden_layers'], \n",
    "                    alpha=params['alpha'],\n",
    "                    activ_type = activ_type)\n",
    "        \n",
    "        \n",
    "        # Allena il modello e ottieni la cronologia (history)\n",
    "        history = fit(data_loader=data_loader, model=model, learning_rate=params['learning_rate'], \n",
    "                      weight_decay=params['regularization'], momentum=params['momentum'],\n",
    "                      epochs=params['epochs'], patience = params['patience'], optim_type = optim_type, reg_flag = reg_flag, val_data=X_val, val_labels=y_val)\n",
    "        \n",
    "\n",
    "        # Prendi il miglior score (l'accuratezza di validazione massima)\n",
    "        score = max(history['val_accuracy'])\n",
    "        accuracy_per_fold.append(score)\n",
    "        fold_no += 1\n",
    "\n",
    "    avg_score = np.mean(accuracy_per_fold)\n",
    "\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    dataset = CustomDataset(X_train, y_train)\n",
    "    data_loader = DataLoader(dataset, batch_size=int(params['batch_size']), shuffle=False)\n",
    "\n",
    "\n",
    "    model = NN(input_size = input_size, hidden_size = params['hidden_size'], output_size = output_size,\n",
    "               hidden_layers = params['hidden_layers'], alpha=params['alpha'], activ_type = activ_type)\n",
    "        \n",
    "    \n",
    "    history = fit(data_loader=data_loader, model=model, learning_rate=params['learning_rate'], \n",
    "                      weight_decay=params['regularization'], momentum=params['momentum'],\n",
    "                      epochs=params['epochs'], patience = params['patience'], optim_type = optim_type, reg_flag = reg_flag, val_data=X_val, val_labels=y_val)\n",
    "\n",
    "\n",
    "    return avg_score, history, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "encoder = OneHotEncoder(categories='auto', sparse_output=False)\n",
    "\n",
    "# Carica i file di addestramento e test per ciascun dataset dal percorso specificato\n",
    "monk1_train = pd.read_csv('../Datasets/Monks/monks-1.train', sep='\\s+', header=None)\n",
    "monk1_test = pd.read_csv('../Datasets/Monks/monks-1.test', sep='\\s+', header=None)\n",
    "\n",
    "monk2_train = pd.read_csv('../Datasets/Monks/monks-2.train', sep='\\s+', header=None)\n",
    "monk2_test = pd.read_csv('../Datasets/Monks/monks-2.test', sep='\\s+', header=None)\n",
    "\n",
    "monk3_train = pd.read_csv('../Datasets/Monks/monks-3.train', sep='\\s+', header=None)\n",
    "monk3_test = pd.read_csv('../Datasets/Monks/monks-3.test', sep='\\s+', header=None)\n",
    "\n",
    "\n",
    "# Lista per memorizzare i dataset trasformati\n",
    "monks_train = []\n",
    "monks_test = []\n",
    "\n",
    "\n",
    "# Dataset monk1\n",
    "X1_train = monk1_train.iloc[:, 1:7].values  # Caratteristiche\n",
    "y1_train = monk1_train.iloc[:, 0].values    # Etichette\n",
    "\n",
    "X1_test = monk1_test.iloc[:, 1:7].values\n",
    "y1_test = monk1_test.iloc[:, 0].values\n",
    "\n",
    "# Applicazione dell'encoder a monk1\n",
    "X1_train_encoded = encoder.fit_transform(X1_train)  # Fit e trasformazione sui dati di training\n",
    "X1_test_encoded = encoder.transform(X1_test)        # Solo trasformazione sui dati di test\n",
    "\n",
    "monks_train.append((X1_train_encoded, y1_train))\n",
    "monks_test.append((X1_test_encoded, y1_test))\n",
    "\n",
    "# Dataset monk2\n",
    "X2_train = monk2_train.iloc[:, 1:7].values\n",
    "y2_train = monk2_train.iloc[:, 0].values\n",
    "\n",
    "X2_test = monk2_test.iloc[:, 1:7].values\n",
    "y2_test = monk2_test.iloc[:, 0].values\n",
    "\n",
    "# Applicazione dell'encoder a monk2\n",
    "X2_train_encoded = encoder.fit_transform(X2_train)\n",
    "X2_test_encoded = encoder.transform(X2_test)\n",
    "\n",
    "monks_train.append((X2_train_encoded, y2_train))\n",
    "monks_test.append((X2_test_encoded, y2_test))\n",
    "\n",
    "# Dataset monk3\n",
    "X3_train = monk3_train.iloc[:, 1:7].values\n",
    "y3_train = monk3_train.iloc[:, 0].values\n",
    "\n",
    "X3_test = monk3_test.iloc[:, 1:7].values\n",
    "y3_test = monk3_test.iloc[:, 0].values\n",
    "\n",
    "# Applicazione dell'encoder a monk3\n",
    "X3_train_encoded = encoder.fit_transform(X3_train)\n",
    "X3_test_encoded = encoder.transform(X3_test)\n",
    "\n",
    "monks_train.append((X3_train_encoded, y3_train))\n",
    "monks_test.append((X3_test_encoded, y3_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greed_search(data, labels, activ_type = 'tanh', optim_type = 'SGD', reg_flag = False, input_size = 6, output_size = 1, param_grid = None):\n",
    "    best_scores = []  # Usa una lista normale per memorizzare i punteggi\n",
    "    best_params_list = []  # Lista per le configurazioni\n",
    "    best_models = []  # Lista per i modelli\n",
    "    best_histories = []  # Lista per la storia di allenamento\n",
    "\n",
    "\n",
    "    for params in param_grid:\n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(f\"Testing params: {params}\")\n",
    "        score, history, model = k_fold_cross_validation(data, labels, activ_type, optim_type, reg_flag, input_size = input_size, output_size=output_size, params=params)\n",
    "        print(f\"Score : {score}\")\n",
    "\n",
    "        # Aggiungi i risultati alla lista\n",
    "        best_scores.append(score)\n",
    "        best_params_list.append(params)\n",
    "        best_models.append(model)\n",
    "        best_histories.append(history)\n",
    "\n",
    "        # Ordina la lista dei punteggi e mantieni solo i migliori 10\n",
    "        sorted_indices = np.argsort(best_scores)[::-1]  # Ordina i punteggi in ordine decrescente\n",
    "        best_scores = [best_scores[i] for i in sorted_indices][:10]  # Usa la lista e mantieni i top 10\n",
    "        best_params_list = [best_params_list[i] for i in sorted_indices][:10]\n",
    "        best_models = [best_models[i] for i in sorted_indices][:10]\n",
    "        best_histories = [best_histories[i] for i in sorted_indices][:10]\n",
    "\n",
    "    print(\"--------------------END GREED SEARCH------------------------------\")\n",
    "\n",
    "    # Ora hai i 10 migliori risultati\n",
    "    print(\"Top 10 best scores:\")\n",
    "    print(best_scores)\n",
    "    print(\"Top 10 best params:\")\n",
    "    print(best_params_list)\n",
    "\n",
    "    return best_scores, best_params_list, best_models, best_histories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generazione delle combinazioni di iperparametri...\n",
      "--------------------------------------------------MONK 1--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Testing params: {'learning_rate': 0.4, 'epochs': 180, 'batch_size': 4, 'hidden_size': 3, 'hidden_layers': 1, 'momentum': 0.3, 'regularization': 0.001, 'alpha': 0.01, 'patience': 30}\n",
      "52.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m param_grid_monk1 \u001b[38;5;241m=\u001b[39m generate_hyperparameter_combinations(param_ranges_1)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--------------------------------------------------MONK 1--------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m best_scores[\u001b[38;5;241m0\u001b[39m], best_params_list[\u001b[38;5;241m0\u001b[39m], best_models[\u001b[38;5;241m0\u001b[39m], best_histories[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m greed_search(X1_train_encoded, y1_train, \n\u001b[1;32m     31\u001b[0m                                                                                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSGD\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m     32\u001b[0m                                                                                       input_size, output_size, param_grid_monk1)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--------------------------------------------------Plots--------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (history, params, score) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(best_histories[\u001b[38;5;241m0\u001b[39m], best_params_list[\u001b[38;5;241m0\u001b[39m], best_scores[\u001b[38;5;241m0\u001b[39m])):\n",
      "Cell \u001b[0;32mIn[66], line 11\u001b[0m, in \u001b[0;36mgreed_search\u001b[0;34m(data, labels, activ_type, optim_type, reg_flag, input_size, output_size, param_grid)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m score, history, model \u001b[38;5;241m=\u001b[39m k_fold_cross_validation(data, labels, activ_type, optim_type, reg_flag, input_size \u001b[38;5;241m=\u001b[39m input_size, output_size\u001b[38;5;241m=\u001b[39moutput_size, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Aggiungi i risultati alla lista\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[64], line 28\u001b[0m, in \u001b[0;36mk_fold_cross_validation\u001b[0;34m(data, labels, activ_type, optim_type, reg_flag, input_size, output_size, params)\u001b[0m\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m NN(input_size \u001b[38;5;241m=\u001b[39m input_size,\n\u001b[1;32m     20\u001b[0m             hidden_size \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     21\u001b[0m             output_size \u001b[38;5;241m=\u001b[39m output_size,\n\u001b[1;32m     22\u001b[0m             hidden_layers \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_layers\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     23\u001b[0m             alpha\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     24\u001b[0m             activ_type \u001b[38;5;241m=\u001b[39m activ_type)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Allena il modello e ottieni la cronologia (history)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m history \u001b[38;5;241m=\u001b[39m fit(data_loader\u001b[38;5;241m=\u001b[39mdata_loader, model\u001b[38;5;241m=\u001b[39mmodel, learning_rate\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     29\u001b[0m               weight_decay\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregularization\u001b[39m\u001b[38;5;124m'\u001b[39m], momentum\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmomentum\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     30\u001b[0m               epochs\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m], patience \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatience\u001b[39m\u001b[38;5;124m'\u001b[39m], optim_type \u001b[38;5;241m=\u001b[39m optim_type, reg_flag \u001b[38;5;241m=\u001b[39m reg_flag, val_data\u001b[38;5;241m=\u001b[39mX_val, val_labels\u001b[38;5;241m=\u001b[39my_val)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Prendi il miglior score (l'accuratezza di validazione massima)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[62], line 67\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(data_loader, model, learning_rate, momentum, weight_decay, epochs, patience, optim_type, reg_flag, val_data, val_labels)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#validazione di quell'epoca\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m val_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(val_data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(val_labels) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 67\u001b[0m     result \u001b[38;5;241m=\u001b[39m evaluation(model, val_data, val_labels, criterion)\n\u001b[1;32m     68\u001b[0m     history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(result[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     69\u001b[0m     history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(result[\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[62], line 91\u001b[0m, in \u001b[0;36mevaluation\u001b[0;34m(model, X, Y, criterion)\u001b[0m\n\u001b[1;32m     89\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m correct_predictions \u001b[38;5;241m/\u001b[39m total_samples\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy)\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [loss\u001b[38;5;241m.\u001b[39mitem(),accuracy\u001b[38;5;241m.\u001b[39mitem()]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "best_scores = [[], [], []]  # Usa una lista di tre elementi per memorizzare i punteggi\n",
    "best_params_list = [[], [], []]  # Lista di tre elementi per le configurazioni\n",
    "best_models = [[], [], []]  # Lista di tre elementi per i modelli\n",
    "best_histories = [[], [], []]  # Lista di tre elementi per la storia di allenamento\n",
    "\n",
    "# Definisce i parametri della rete neurale\n",
    "input_size = X1_train_encoded.shape[1]\n",
    "output_size = 1\n",
    "\n",
    "# Definizione dei range degli iperparametri\n",
    "param_ranges_1 = {\n",
    "    \"learning_rate\": (0.4, 0.7, 0.1),  # Da 0.01 a 0.5 con step di 0.05\n",
    "    \"epochs\": (180, 180, 1),                   # Da 0 a 10 con step di 1\n",
    "    \"batch_size\": (4, 4, 1),             # Da 8 a 32 con step di 8\n",
    "    \"hidden_size\": (3, 3, 1),           # Da 32 a 128 con step di 32\n",
    "    \"hidden_layers\": (1, 1, 1),             # Da 1 a 3 con step di 1\n",
    "    \"momentum\": (0.3, 0.9, 0.1),          # Da 0.9 a 0.99 con step di 0.01\n",
    "    \"regularization\": (0.001, 0.001, 0.005),             # Da 0.0 a 0.1 con step di 0.05\n",
    "    \"alpha\": (0.01, 0.01, 0.01),             # Da 0.01 a 0.1 con step di 0.05\n",
    "    \"patience\": (30, 30, 1)            # Da 5 a 15 con step di 5\n",
    "}\n",
    "\n",
    "#start_time = time.time()\n",
    "print(\"Generazione delle combinazioni di iperparametri...\")\n",
    "param_grid_monk1 = []\n",
    "param_grid_monk1 = generate_hyperparameter_combinations(param_ranges_1)\n",
    "\n",
    "\n",
    "print(\"--------------------------------------------------MONK 1--------------------------------------------------\")\n",
    "best_scores[0], best_params_list[0], best_models[0], best_histories[0] = greed_search(X1_train_encoded, y1_train, \n",
    "                                                                                      'tanh', 'SGD', False, \n",
    "                                                                                      input_size, output_size, param_grid_monk1)\n",
    "\n",
    "print(\"--------------------------------------------------Plots--------------------------------------------------\")\n",
    "for i, (history, params, score) in enumerate(zip(best_histories[0], best_params_list[0], best_scores[0])):\n",
    "    print(f\"{params}\")\n",
    "    print(f\"Score: {score}\")\n",
    "    plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generazione delle combinazioni di iperparametri...\n",
      "--------------------------------------------------MONK 2--------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m param_grid_monk2 \u001b[38;5;241m=\u001b[39m generate_hyperparameter_combinations(param_ranges_2)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--------------------------------------------------MONK 2--------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m best_scores[\u001b[38;5;241m1\u001b[39m], best_params_list[\u001b[38;5;241m1\u001b[39m], best_models[\u001b[38;5;241m1\u001b[39m], best_histories[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m greed_search(X2_train_encoded, y2_train, \n\u001b[1;32m     26\u001b[0m                                                                                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSGD\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m     27\u001b[0m                                                                                       input_size, param_grid_monk2)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--------------------------------------------------Plots--------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (history, params, score) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(best_histories[\u001b[38;5;241m1\u001b[39m], best_params_list[\u001b[38;5;241m1\u001b[39m], best_scores[\u001b[38;5;241m1\u001b[39m])):\n",
      "Cell \u001b[0;32mIn[52], line 8\u001b[0m, in \u001b[0;36mgreed_search\u001b[0;34m(data, labels, activ_type, optim_type, reg_flag, input_size, output_size, param_grid)\u001b[0m\n\u001b[1;32m      4\u001b[0m best_models \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# Lista per i modelli\u001b[39;00m\n\u001b[1;32m      5\u001b[0m best_histories \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# Lista per la storia di allenamento\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m params \u001b[38;5;129;01min\u001b[39;00m param_grid:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Definisce i parametri della rete neurale\n",
    "input_size = X2_train_encoded.shape[1]\n",
    "output_size = 1\n",
    "\n",
    "# Definizione dei range degli iperparametri\n",
    "param_ranges_2 = {\n",
    "    \"learning_rate\": (0.4, 0.9, 0.1),  # Da 0.01 a 0.5 con step di 0.05\n",
    "    \"epochs\": (180, 180, 1),                   # Da 0 a 10 con step di 1\n",
    "    \"batch_size\": (4, 16, 6),             # Da 8 a 32 con step di 8\n",
    "    \"hidden_size\": (3, 3, 1),           # Da 32 a 128 con step di 32\n",
    "    \"hidden_layers\": (1, 1, 1),             # Da 1 a 3 con step di 1\n",
    "    \"momentum\": (0.3, 0.6, 0.1),          # Da 0.9 a 0.99 con step di 0.01\n",
    "    \"regularization\": (0.001, 0.001, 0.005),             # Da 0.0 a 0.1 con step di 0.05\n",
    "    \"alpha\": (0.01, 0.01, 0.01),             # Da 0.01 a 0.1 con step di 0.05\n",
    "    \"patience\": (30, 30, 1)            # Da 5 a 15 con step di 5\n",
    "}\n",
    "\n",
    "#start_time = time.time()\n",
    "print(\"Generazione delle combinazioni di iperparametri...\")\n",
    "param_grid_monk2 = []\n",
    "param_grid_monk2 = generate_hyperparameter_combinations(param_ranges_2)\n",
    "\n",
    "\n",
    "print(\"--------------------------------------------------MONK 2--------------------------------------------------\")\n",
    "best_scores[1], best_params_list[1], best_models[1], best_histories[1] = greed_search(X2_train_encoded, y2_train, \n",
    "                                                                                      'tanh', 'SGD', False, \n",
    "                                                                                      input_size, output_size, param_grid_monk2)\n",
    "\n",
    "print(\"--------------------------------------------------Plots--------------------------------------------------\")\n",
    "for i, (history, params, score) in enumerate(zip(best_histories[1], best_params_list[1], best_scores[1])):\n",
    "    print(f\"{params}\")\n",
    "    print(f\"Score: {score}\")\n",
    "    plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisce i parametri della rete neurale\n",
    "input_size = X3_train_encoded.shape[1]\n",
    "output_size = 1\n",
    "\n",
    "# Definizione dei range degli iperparametri\n",
    "param_ranges_3 = {\n",
    "    \"learning_rate\": (0.1, 0.4, 0.1),  # Da 0.01 a 0.5 con step di 0.05\n",
    "    \"epochs\": (180, 180, 1),                   # Da 0 a 10 con step di 1\n",
    "    \"batch_size\": (4, 16, 6),             # Da 8 a 32 con step di 8\n",
    "    \"hidden_size\": (2, 3, 1),           # Da 32 a 128 con step di 32\n",
    "    \"hidden_layers\": (1, 1, 1),             # Da 1 a 3 con step di 1\n",
    "    \"momentum\": (0.3, 0.9, 0.1),          # Da 0.9 a 0.99 con step di 0.01\n",
    "    \"regularization\": (0.1, 0.4, 0.2),             # Da 0.0 a 0.1 con step di 0.05\n",
    "    \"alpha\": (0.01, 0.01, 0.01),             # Da 0.01 a 0.1 con step di 0.05\n",
    "    \"patience\": (30, 30, 1)            # Da 5 a 15 con step di 5\n",
    "}\n",
    "\n",
    "#start_time = time.time()\n",
    "print(\"Generazione delle combinazioni di iperparametri...\")\n",
    "param_grid_monk3 = []\n",
    "param_grid_monk3 = generate_hyperparameter_combinations(param_ranges_3)\n",
    "\n",
    "\n",
    "print(\"--------------------------------------------------MONK 1--------------------------------------------------\")\n",
    "best_scores[2], best_params_list[2], best_models[2], best_histories[2] = greed_search(X3_train_encoded, y3_train, \n",
    "                                                                                      'tanh', 'SGD', False, \n",
    "                                                                                      input_size, output_size, param_grid_monk3)\n",
    "\n",
    "print(\"--------------------------------------------------Plots--------------------------------------------------\")\n",
    "for i, (history, params, score) in enumerate(zip(best_histories[2], best_params_list[2], best_scores[2])):\n",
    "    print(f\"{params}\")\n",
    "    print(f\"Score: {score}\")\n",
    "    plot_training_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
